import os
import json
import requests
import streamlit as st
import pandas as pd
import csv
#import pdfplumber
#import ollama
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.callbacks.manager import CallbackManagerForLLMRun
from langchain_core.language_models.llms import LLM
from langchain.globals import set_llm_cache
from langchain_community.vectorstores import Chroma
from langchain_community.cache import InMemoryCache
from docx import Document as DocxDocument
from PyPDF2 import PdfReader
from typing import Any, List, Mapping, Optional
from io import BytesIO
from datetime import datetime

# ì´ˆê¸° ì„¸ì…˜ ìƒíƒœ ì„¤ì •
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "ë¬´ì—‡ì„ ë„ì™€ ë“œë¦´ê¹Œìš”?"}]
if "vectorstore_cache" not in st.session_state:
    st.session_state["vectorstore_cache"] = {}
if "loading_text" not in st.session_state:
    st.session_state["loading_text"] = None
if "retriever" not in st.session_state:
    st.session_state["retriever"] = None
if 'questions' not in st.session_state:
    st.session_state['questions'] = []
if 'answers' not in st.session_state:
    st.session_state['answers'] = []
if "cache_initialized" not in st.session_state:
    set_llm_cache(InMemoryCache())
    st.session_state.cache_initialized = True
if 'chat_history' not in st.session_state:
    st.session_state['chat_history'] = []

# +

# HCX API í´ë˜ìŠ¤
class LlmClovaStudio(LLM):
    """
    Custom LLM class for using the ClovaStudio API.
    """
    host: str
    api_key: str
    api_key_primary_val: str
    request_id: str

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.host = kwargs.get('host')
        self.api_key = kwargs.get('api_key')
        self.api_key_primary_val = kwargs.get('api_key_primary_val')
        self.request_id = kwargs.get('request_id')

    @property
    def _llm_type(self) -> str:
        return "custom"

    def _call(self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None
    ) -> str:
        """
        Make an API call to the ClovaStudio endpoint using the specified 
        prompt and return the response.
        """

        if stop is not None:
            raise ValueError("stop kwargs are not permitted.")

        headers = {
            "X-NCP-CLOVASTUDIO-API-KEY": self.api_key,
            "X-NCP-APIGW-API-KEY": self.api_key_primary_val,
            "X-NCP-CLOVASTUDIO-REQUEST-ID": self.request_id,
            "Content-Type": "application/json; charset=utf-8",
            "Accept": "text/event-stream"
        }

        sys_prompt = """
        ë‹¹ì‹ ì€ AKì•„ì´ì—ìŠ¤ì˜ ì‚¬ë‚´ ê·œì •, ì •ì±…, ë³µë¦¬í›„ìƒ, ì—…ë¬´ ê°€ì´ë“œ ë“±ì— ëŒ€í•´ ë‹µë³€í•˜ëŠ” ì—­í• ì„ ë§¡ê³  ìˆìŠµë‹ˆë‹¤
        - Context ë‚´ì—ì„œë§Œ ë‹µí•˜ë©°, ì¶”ì¸¡í•˜ê±°ë‚˜ ì¶”ê°€ ì •ë³´ ì œê³µ ê¸ˆì§€
        - ê°ì‚¬, ì¹­ì°¬ì— ëŒ€í•œ ë‹µë³€ì€ 'ê°ì‚¬í•©ë‹ˆë‹¤!'ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”. ì¶”ê°€ ì„¤ëª… ê¸ˆì§€
        - Contextì— ì •ë³´ê°€ ì—†ìœ¼ë©´ 'ì˜ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤'ë¼ê³  ë‹µë³€
        """
        #- ì§ˆë¬¸ì— ëŒ€í•´ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ í•µì‹¬ ì •ë³´ë§Œ ì „ë‹¬, ê³µê° í‘œí˜„ì€ ê°€ëŠ¥
        #- íšŒì‚¬, ì—…ë¬´, ë³µì§€(ì„ ë¬¼, íœ´ê°€ ë“±)ê³¼ ê´€ë ¨ ì—†ëŠ” ì§ˆë¬¸ì—ëŠ” "ì£„ì†¡í•©ë‹ˆë‹¤. ì €ëŠ” ì—…ë¬´ ê´€ë ¨ ë‚´ìš©ì—ë§Œ ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."ë¼ê³  ë‹µë³€
        # ìœ„ í”„ë¡¬í”„íŠ¸ ë„£ì„ ê²½ìš° ê³¼ë„í•˜ê²Œ ì—…ë¬´ì™€ ê´€ë ¨ ì—†ëŠ” ì§ˆë¬¸ì´ë¼ê³  íŒë‹¨í•˜ëŠ” ê²½í–¥ ì¡´ì¬(ê°œì„  í•„ìš”)
        
        preset_text = [{"role": "system", "content": sys_prompt}, {"role": "user", "content": prompt}]

        request_data = {
            "messages": preset_text,
            "topP": 0.6,
            "topK": 0,
            "maxTokens": 256,
            "temperature": 0.6,
            "repeatPenalty": 3,
            "stopBefore": [],
            "includeAiFilters": False
        }
        # API ìš”ì²­
        response = requests.post(
            self.host + "/testapp/v1/chat-completions/HCX-003",  # ë³¸ì¸ì´ finetunning í•œ API ê²½ë¡œ , ì¼ë°˜ HCX03ì¨ë„ ë¬´ê´€í•©ë‹ˆë‹¤
            headers=headers,
            json=request_data,
            stream=True
        )

        # ìŠ¤íŠ¸ë¦¼ì—ì„œ ë§ˆì§€ë§‰ 'data:' ë¼ì¸ì„ ì°¾ê¸° ìœ„í•œ ë¡œì§
        last_data_content = ""

        for line in response.iter_lines():
            if line:
                decoded_line = line.decode("utf-8")
                if '"data":"[DONE]"' in decoded_line:
                    break
                if decoded_line.startswith("data:"):
                    last_data_content = json.loads(decoded_line[5:])["message"]["content"]

        return last_data_content

llm = LlmClovaStudio(
    host='https://clovastudio.stream.ntruss.com',
    api_key='NTA0MjU2MWZlZTcxNDJiY6+5UNhJXWh3gqmFLbiMpde7ehpEJAFPwFUIey9lGc0S',
    api_key_primary_val='VhkfehtF14qpXmZPIA6VRw6x1c1eDCXp3P6BfbrG',
    #request_id='1e8ac996-b6e9-45f9-a64f-a64e37a029cf' #HCX-DASH-001
    request_id='b9288b57-8e12-45cc-b378-49cc13d8dbb6' #HCX-003
)

# -

#'Manual' ê²½ë¡œ ë‚´ pdf íŒŒì¼ ë¡œë”©
@st.cache_resource(show_spinner=False)
def extract_text_from_pdfs(folder_path, start_page=None, end_page=None):
    text = ''
    for filename in os.listdir(folder_path):
        if filename.endswith(".pdf"):
            pdf_path = os.path.join(folder_path, filename)
            with open(pdf_path, 'rb') as file:
                reader = PdfReader(file)
                start_page = start_page - 1 if start_page else 0
                end_page = end_page if end_page and end_page <= len(reader.pages) else len(reader.pages)
                for page_num in range(start_page, end_page):
                    text += reader.pages[page_num].extract_text()

    return text

# @st.cache_resource(show_spinner=False)
# def extract_text_from_pdfs(folder_path, start_page=None, end_page=None):
#     all_text = ''
    
#     for filename in os.listdir(folder_path):
#         if filename.endswith(".pdf"):
#             pdf_path = os.path.join(folder_path, filename)
#             try:
#                 with pdfplumber.open(pdf_path) as pdf:
#                     num_pages = len(pdf.pages)
                    
#                     # ì‹œì‘ í˜ì´ì§€ì™€ ë í˜ì´ì§€ ì²˜ë¦¬
#                     start = max(start_page - 1, 0) if start_page else 0
#                     end = min(end_page, num_pages) if end_page else num_pages
                    
#                     # í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
#                     for page_num in range(start, end):
#                         page = pdf.pages[page_num]
#                         page_text = page.extract_text()
#                         if page_text:  # í…ìŠ¤íŠ¸ê°€ ìˆì„ ê²½ìš°ì—ë§Œ ì¶”ê°€
#                             all_text += page_text
#             except Exception as e:
#                 print(f"Error processing {filename}: {e}")
    
#     return all_text

# 'Manual' ê²½ë¡œ ë‚´ docx íŒŒì¼ ë¡œë”©
@st.cache_resource(show_spinner=False)
def extract_text_from_docx(folder_path):
    all_text = ''
    
    # í´ë” ë‚´ íŒŒì¼ë“¤ì„ í™•ì¸
    for filename in os.listdir(folder_path):
        if filename.endswith(".docx"):
            docx_path = os.path.join(folder_path, filename)
            try:
                # docx íŒŒì¼ ì—´ê¸°
                doc = DocxDocument(docx_path)
                
                # ë¬¸ì„œì˜ ëª¨ë“  ë‹¨ë½ì„ ì½ì–´ì™€ì„œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                for para in doc.paragraphs:
                    all_text += para.text + '\n'
            except Exception as e:
                print(f"Error processing {filename}: {e}")
    
    return all_text

# ë²¡í„°DB ì„ë² ë”© ë° retriver ìƒì„±
@st.cache_resource(show_spinner=False)
def retrieve_docs(text, model_index=0):
    vectorstore_path = 'vectorstore_' + str(model_index)
 
    model_list = [
        'bespin-global/klue-sroberta-base-continue-learning-by-mnr',
        'BAAI/bge-m3',
        'All-MiniLM-L6-v2',
        'sentence-transformers/paraphrase-MiniLM-L6-v2'
    ]

    embeddings = HuggingFaceEmbeddings(
        model_name=model_list[model_index],
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True},
    )

    os.makedirs(vectorstore_path, exist_ok=True)

    if os.path.exists(os.path.join(vectorstore_path, "index")):
        vectorstore = Chroma(persist_directory=vectorstore_path, embedding_function=embeddings)
    else:
        docs = [Document(page_content=text)]
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=300)
        splits = text_splitter.split_documents(docs)
        vectorstore = Chroma.from_documents(splits, embeddings, persist_directory=vectorstore_path)
        vectorstore.persist()

    return vectorstore.as_retriever(
        search_type='mmr',
        search_kwargs={'k': 3, 'lambda_mult': 0.5}
        )

# ì¤„ë°”ê¿ˆ formatting
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# RAG í•µì‹¬ í•¨ìˆ˜
def rag_chain(question):
    question_history = "\n".join(str(x) for x in st.session_state['questions'][:-4:-1])
    retrieved_docs = retriever.invoke("\n".join([question, question_history])) # retriever í†µí•œ ë²¡í„°DBì—ì„œ í•„ìš”í•œ ì •ë³´ ê²€ìƒ‰
    formatted_context = format_docs(retrieved_docs) # formatting
    formatted_prompt = f"Question: {question}\n\nContext: {formatted_context}" # API ì „ë‹¬í•  Input ìƒì„±
    response = llm.invoke(formatted_prompt) # API í†µí•´ ë‹µë³€ ìƒì„±
    save_chat_to_xlsx(question, response, formatted_prompt) # ì±„íŒ… ê¸°ë¡ ì €ì¥
    return response

# -

# ì±„íŒ… ê¸°ë¡ì„ CSV íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜
def save_chat_to_csv(question, response, formatted_prompt):
    with open('chat_history.csv', mode='a', newline='', encoding='utf-8') as file:
    #with open('chat_history.csv', mode='a', newline='') as file:    
        writer = csv.writer(file)
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        writer.writerow([timestamp, question, response, formatted_prompt])

def save_chat_to_xlsx(question, response, formatted_prompt):
    file_path = 'chat_history.xlsx'
    
    # ìƒˆë¡œìš´ ë°ì´í„° í–‰
    new_data = {
        'Timestamp': [datetime.now().strftime("%Y-%m-%d %H:%M:%S")],
        'Question': [question],
        'Response': [response],
        'Formatted Prompt': [formatted_prompt]
    }
    
    # ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
    new_df = pd.DataFrame(new_data)
    
    # íŒŒì¼ì´ ì´ë¯¸ ìˆëŠ”ì§€ í™•ì¸í•˜ì—¬ ì²˜ë¦¬
    if os.path.exists(file_path):
        # ê¸°ì¡´ Excel íŒŒì¼ì— ë°ì´í„° ì¶”ê°€
        existing_df = pd.read_excel(file_path)
        combined_df = pd.concat([existing_df, new_df], ignore_index=True)
    else:
        # ìƒˆë¡œìš´ Excel íŒŒì¼ ìƒì„±
        combined_df = new_df

    combined_df.to_excel(file_path, index=False)

# -

if __name__ == "__main__":
    st.title("ğŸ“Ÿì±—ë´‡ ì•„í‚¤ì…ë‹ˆë‹¤ğŸ“Ÿ")
    st.markdown("AKì•„ì´ì—ìŠ¤ì˜ ê·œì •, ë³µì§€ì œë„ì— ëŒ€í•´ ëŒ€ë‹µí•  ìˆ˜ ìˆì–´ìš”.")
    st.markdown("í˜„ì¬ ì±—ë´‡ì€ **í…ŒìŠ¤íŠ¸ìš©**ìœ¼ë¡œ ìš´ì˜ë˜ê³  ìˆìŠµë‹ˆë‹¤. ì•„ê»´ì£¼ì„¸ìš”.")
    with st.spinner('ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.'):
        loading_text = extract_text_from_docx("Manual") # ë§¤ë‰´ì–¼ ë¡œë”©
        loading_text = loading_text + extract_text_from_pdfs("Manual")
        retriever = retrieve_docs(loading_text) # retriever ìƒì„±

    for msg in st.session_state.messages:
        st.chat_message(msg["role"]).write(msg["content"])

    if prompt := st.chat_input():
        prompt=prompt[:100]
        st.session_state.messages.append({"role": "user", "content": prompt})
        st.session_state['questions'].append(prompt)
        st.chat_message("user").write(prompt)
        with st.spinner(''):
            msg = rag_chain(prompt)
        st.session_state.messages.append({"role": "assistant", "content": msg})
        st.chat_message("assistant").write(msg)
        
#terminal ì‹¤í–‰ ì»¤ë§¨ë“œ
#streamlit run app.py